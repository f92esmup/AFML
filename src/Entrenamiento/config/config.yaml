portafolio:
  capital_inicial: 10000.0
  apalancamiento: 1.0
  comision: 0.001
  slippage: 0.001

entorno:
  window_size: 30
  max_drawdown_permitido: 0.2
  factor_aversion_riesgo: 2
  umbral_mantener_posicion: 0.05
  penalizacion_no_operar: 0.1
  episodios: 0

SACmodel:
  policy: "MlpPolicy"
  # --- Hiperparámetros principales de optimización ---
  learning_rate: 3e-4
  buffer_size: 1000000
  learning_starts: 5000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  ent_coef: "auto_0.1"
  # --- Frecuencia de entrenamiento ---
  train_freq: [1, "step"]
  gradient_steps: 1
  # --- Reproducibilidad y monitorización ---
  verbose: 1
  seed: 42

policy_kwargs:
  net_arch:
    pi: [256, 256]
    qf: [256, 256]
  log_std_init: -3.0
  n_critics: 2

Vecnormalize:
  norm_obs: true
  norm_reward: true
  clip_obs: 10.0
  gamma: 0.99
