# RefactorizaciÃ³n: Flujo Unificado de Entrenamiento

**Fecha:** 4 de Octubre de 2025  
**Branch:** `refactor/unified-training-flow`  
**Estado:** âœ… Completado

## ğŸ“‹ Resumen Ejecutivo

Se ha completado una refactorizaciÃ³n mayor del sistema de entrenamiento para eliminar la separaciÃ³n entre descarga de datos (`create_dataset.py`) y entrenamiento (`train.py`). Ahora existe un **Ãºnico flujo unificado** que implementa un paso completo de walk-forward:

```
Descarga datos train â†’ Entrena â†’ Descarga datos eval â†’ EvalÃºa â†’ Guarda metadata
```

## ğŸ¯ Objetivos Cumplidos

âœ… Eliminar concepto de `data_id` (datasets pre-generados)  
âœ… Integrar descarga de datos directamente en `train.py`  
âœ… Usar solo `train_id` para identificar experimentos  
âœ… Guardar scalers en directorio de entrenamiento (no en datasets/)  
âœ… Guardar toda la configuraciÃ³n `UnifiedConfig` en metadata  
âœ… Implementar gestiÃ³n explÃ­cita de memoria entre fases  
âœ… Usar scaler de train en evaluaciÃ³n (consistencia con producciÃ³n)  

## ğŸ“ Archivos Modificados/Eliminados

### Eliminados
- âŒ `create_dataset.py` - Funcionalidad integrada en train.py
- âŒ `src/train/AdquisicionDatos/pipeline.py` - Ya no necesario
- âŒ `DatasetConfig` class - Concepto de dataset eliminado

### Modificados
- âœï¸ `train.py` - RefactorizaciÃ³n completa (270 lÃ­neas modificadas)
- âœï¸ `src/train/config/cli.py` - Nuevos argumentos CLI
- âœï¸ `src/train/config/config.py` - Nuevas clases y mÃ©todos
- âœï¸ `src/train/AdquisicionDatos/__init__.py` - Exports actualizados
- âœï¸ `README.md` - DocumentaciÃ³n actualizada

## ğŸ”„ Cambios en CLI

### Antes (obsoleto)
```bash
# 1. Crear dataset
python create_dataset.py \
  --symbol BTCUSDT \
  --interval 1h \
  --start-date 2024-01-01 \
  --end-date 2024-06-30

# 2. Entrenar con dataset
python train.py \
  --data-id BTCUSDT_20241004_123456 \
  --data-eval-id BTCUSDT_20241004_234567 \
  --episodios 5
```

### Ahora (nuevo flujo)
```bash
python train.py \
  --symbol BTCUSDT \
  --interval 1h \
  --train-start-date 2024-01-01 \
  --train-end-date 2024-06-30 \
  --eval-start-date 2024-07-01 \
  --eval-end-date 2024-09-30 \
  --total-timesteps 50000 \
  --episodios-eval 3
```

**Argumentos eliminados:**
- `--data-id`
- `--data-eval-id`
- `--episodios` (reemplazado por `--total-timesteps`)

**Argumentos nuevos:**
- `--symbol` (requerido)
- `--interval` (requerido)
- `--train-start-date` (requerido)
- `--train-end-date` (requerido)
- `--eval-start-date` (requerido)
- `--eval-end-date` (requerido)
- `--total-timesteps` (opcional, default: 10000) - Especifica directamente los pasos de entrenamiento

## ğŸ—ï¸ Cambios en Estructura de Directorios

### Antes
```
datasets/
  BTCUSDT_20241003_235804/
    data.csv          â† guardado en disco
    metadata.yaml
    scaler.pkl

entrenamientos/
  train_BTCUSDT_20241003_235804_lr3e-4_bs256_ws30_20241004_000314/
    config_metadata.yaml
    modelos/modelo.zip
    evaluacion/...
    tensorboard/...
```

### Ahora
```
entrenamientos/
  train_BTCUSDT_20240101_20240630_lr3e-4_bs256_ws30_20241004_143022/
    config_metadata.yaml       â† TODO UnifiedConfig + metadata ejecuciÃ³n
    scaler_train.pkl           â† Scaler de entrenamiento
    scaler_eval.pkl            â† Scaler de evaluaciÃ³n (opcional)
    modelos/modelo.zip
    evaluacion/...
    tensorboard/...
```

**Cambios clave:**
- âŒ Carpeta `datasets/` ya no se usa
- âœ… Scalers guardados en carpeta de entrenamiento
- âœ… Nuevo formato de `train_id` incluye fechas: `train_{symbol}_{start}_{end}_...`
- âœ… Metadata completa (toda la configuraciÃ³n) en un solo archivo

## ğŸ”§ Cambios en config.py

### Clases Eliminadas
```python
class OutputDataAcquisitionConfig  # Ya no necesaria
class OutputTrainingConfig         # Ya no necesaria  
class DatasetConfig                # Concepto eliminado
```

### Clases Nuevas/Modificadas
```python
class OutputConfig(BaseModel):
    """ConfiguraciÃ³n de salida unificada."""
    base_dir: str
    model_path: str
    tensorboard_log: str
    scaler_train_path: str        # NUEVO
    scaler_eval_path: str          # NUEVO
    metadata_filename: str

class DataDownloaderConfig(BaseModel):
    # Campos ahora son REQUERIDOS (no Optional)
    symbol: str                    # antes: Optional[str]
    interval: str                  # antes: Optional[str]
    start_date: str                # antes: Optional[str]
    end_date: str                  # antes: Optional[str]
```

### MÃ©todos Eliminados
```python
UnifiedConfig.load_for_data_acquisition()  # Ya no usado
UnifiedConfig._add_dataset_info()          # Ya no necesario
UnifiedConfig._data_id()                   # Concepto eliminado
UnifiedConfig._train_id()                  # Reemplazado
```

### MÃ©todos Nuevos
```python
UnifiedConfig.load_for_unified_training(args)      # Carga config para flujo unificado
UnifiedConfig._add_cli_args_unified(args, yaml)    # Integra args CLI
UnifiedConfig._generate_train_id(...)             # Nuevo formato de train_id
UnifiedConfig._add_output_paths_unified(...)      # Configura rutas de salida
```

## ğŸ”„ Cambios en train.py

### Clase `Entrenamiento` - Nuevos MÃ©todos

```python
def _descargar_y_preprocesar(start_date, end_date) -> Tuple[pd.DataFrame, StandardScaler]:
    """Descarga y preprocesa datos para un rango de fechas."""
    # 1. Actualizar config con fechas
    # 2. Descargar datos (DataDownloader)
    # 3. Preprocesar (Preprocesamiento)
    # 4. Retornar (data, scaler)

def _guardar_scaler(scaler, filepath):
    """Guarda el scaler en la ruta especificada."""
    
def entrenar():
    """Entrena con descarga integrada."""
    # 1. Descargar datos train
    # 2. Guardar scaler_train
    # 3. Crear entorno + agente
    # 4. Entrenar
    # 5. Guardar modelo
    # 6. LIBERAR MEMORIA (del, gc.collect)
    
def evaluar():
    """EvalÃºa con descarga integrada."""
    # 1. Descargar datos eval
    # 2. Guardar scaler_eval (opcional)
    # 3. Cargar scaler_train
    # 4. Crear entorno eval (con train_scaler)
    # 5. Evaluar
    # 6. LIBERAR MEMORIA

def _guardar_metadata():
    """Guarda TODA la config + metadata de ejecuciÃ³n."""
    # Serializa UnifiedConfig.model_dump()
    # AÃ±ade metadata de ejecuciÃ³n
```

### MÃ©todos Eliminados
```python
_cargar_datos(data_id)           # Ya no carga de datasets/
_cargar_scaler(data_id)          # Ya no carga de datasets/
```

## ğŸ§  Decisiones de DiseÃ±o Importantes

### 1. Scaler en EvaluaciÃ³n
**DecisiÃ³n:** Usar `scaler_train` en el entorno de evaluaciÃ³n  
**RazÃ³n:** Consistencia con producciÃ³n (donde solo tendremos el scaler de train)  
**ImplementaciÃ³n:**
```python
# En evaluar()
train_scaler = joblib.load(self.config.Output.scaler_train_path)
eval_env = TradingEnv(..., scaler=train_scaler)  # NO eval_scaler
```

### 2. GestiÃ³n de Memoria
**DecisiÃ³n:** Liberar explÃ­citamente memoria entre fases  
**RazÃ³n:** Evitar out-of-memory con datasets grandes  
**ImplementaciÃ³n:**
```python
# DespuÃ©s de entrenar
del train_data
del entorno_train
del train_scaler
gc.collect()

# DespuÃ©s de evaluar
del eval_data
del eval_env
del eval_scaler
gc.collect()
```

### 3. Metadata Completa
**DecisiÃ³n:** Guardar TODO `UnifiedConfig` en metadata  
**RazÃ³n:** Trazabilidad completa, reproducibilidad  
**ImplementaciÃ³n:**
```python
config_dict = self.config.model_dump()  # Serializa TODO
config_dict["metadata_execution"] = {...}  # AÃ±ade info de ejecuciÃ³n
```

### 4. ValidaciÃ³n de Fechas Walk-Forward
**DecisiÃ³n:** Validar que eval sea posterior a train  
**RazÃ³n:** Prevenir data leakage en walk-forward  
**ImplementaciÃ³n:**
```python
if train_end >= eval_start:
    raise ValueError("eval debe ser posterior a train")
```

## ğŸ“Š Impacto en el Flujo de Trabajo

### Ventajas
âœ… Menos pasos manuales (1 comando vs 2)  
âœ… No necesidad de gestionar datasets/ manualmente  
âœ… Menos uso de disco (no se guardan DataFrames)  
âœ… Trazabilidad completa en un solo directorio  
âœ… Consistencia: mismo scaler en eval que en producciÃ³n  
âœ… Walk-forward validado automÃ¡ticamente  

### Consideraciones
âš ï¸ Descarga de datos en cada ejecuciÃ³n (mÃ¡s tiempo inicial)  
âš ï¸ Requiere conexiÃ³n a internet para descargar datos  
âš ï¸ No hay reutilizaciÃ³n de datos entre experimentos  

### Mitigaciones
- Datos se descargan una vez por fase (train/eval)
- Descarga es relativamente rÃ¡pida con API de Binance
- Beneficio de memoria y simplicidad compensa el tiempo

## ğŸ§ª Testing Recomendado

### Tests Unitarios a Actualizar
- [ ] `test_UnifiedConfig` - Nuevos mÃ©todos
- [ ] `test_cli_args` - Nuevos argumentos
- [ ] `test_train_id_generation` - Nuevo formato

### Tests de IntegraciÃ³n
- [ ] Test flujo completo con datos pequeÃ±os (1 dÃ­a train, 1 dÃ­a eval)
- [ ] Verificar estructura de directorios generada
- [ ] Verificar contenido de metadata (todas las claves presentes)
- [ ] Verificar que scalers se guardan correctamente
- [ ] Test de liberaciÃ³n de memoria (no debe crecer indefinidamente)

### Tests Manuales Realizados
âœ… VerificaciÃ³n de sintaxis (no errors con get_errors)  
âœ… Commits atÃ³micos con mensajes descriptivos  
âœ… ActualizaciÃ³n de documentaciÃ³n (README)  

## ğŸ“ PrÃ³ximos Pasos

### Pendientes (No Bloqueantes)
- [ ] Actualizar tests existentes para nueva estructura
- [ ] Crear tests para flujo unificado
- [ ] Opcional: AÃ±adir cache de datos descargados (si se necesita)
- [ ] Opcional: Soporte para mÃºltiples sÃ­mbolos en paralelo

### Para ProducciÃ³n
- [ ] Validar con datos reales (semanas/meses de datos)
- [ ] Monitorear uso de memoria en entrenamiento largo
- [ ] Crear guÃ­a de troubleshooting
- [ ] Documentar casos de uso avanzados

## ğŸ”— Commits Relacionados

```
9cdfc8f - refactor(config): actualizar CLI y config para flujo unificado de entrenamiento
e21d26c - refactor(train): implementar flujo unificado de entrenamiento  
7c0f064 - chore: eliminar create_dataset.py obsoleto
8df0d05 - docs: actualizar README con instrucciones del flujo unificado
35d4bf2 - refactor: eliminar pipeline.py y actualizar exports
```

## ğŸ“š Referencias

- Issue/DiscusiÃ³n: ConversaciÃ³n con IA sobre refactorizaciÃ³n
- DocumentaciÃ³n anterior: README.md (versiÃ³n anterior)
- Flujo anterior: `create_dataset.py` + `train.py` (preservado en `train.py.backup`)

---

**Autor:** Pedro (con asistencia de GitHub Copilot)  
**RevisiÃ³n:** Pendiente  
**AprobaciÃ³n:** Pendiente merge a `dev`
